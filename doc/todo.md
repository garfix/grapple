# To do

- build original GPT-3 transformer
  - byte-pair encoding
  - input embedding
  - positional encoding
  - encoder stack
  - decoder stack

## Input embedding

- skip-gram architecture of the word2vec
- a skip-gram model generally contains an input layer, weights, a hidden layer, and an output containing the word embeddings of the tokenized words

Input: encoded words
Output: n-dim vector of floats


https://pournejatian.medium.com/the-math-behind-word2vec-skip-gram-implementation-ab9995730b7e

http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

