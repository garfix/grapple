## Information

Some information resources I found useful

## Back-propagation

A very clear explanation of back-propagation: https://hmkcode.com/ai/backpropagation-step-by-step/

## Cosine similarity



## Byte-pair encoding (BPE)

Wikipedia: https://en.wikipedia.org/wiki/Byte_pair_encoding

Stack exchange article on how the transformer uses BPE: https://stats.stackexchange.com/questions/469226/how-does-transformer-use-bpe

Code examples: https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0


## Word2vec - skip-gram

High-level overview: https://medium.datadriveninvestor.com/word2vec-skip-gram-model-explained-383fa6ddc4ae

More in-depth: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

